{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Domain-Invariant Fake News Detection </h1>\n",
    "<img src=\"images/fake_news.gif\" alt=\"Fake News\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, LSTM, Embedding, Reshape\n",
    "from keras.models import load_model, model_from_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Preprocessed Dataset\n",
    "We crawled over webistes to get real and fake news dataset.<br>\n",
    "The dataset is divided into 5 different categories which is :\n",
    "- India\n",
    "- Politics\n",
    "- Entertainment\n",
    "- Sports and\n",
    "- Technology\n",
    "\n",
    "The preprocessing takes the raw dataset and performs following operations\n",
    "- Lower the text.\n",
    "- Remove Quotes\n",
    "- Remove all the special characters\n",
    "- Replace multiple spaces with one space\n",
    "- Tokenize the Words\n",
    "\n",
    "We are using Pandas to read the dataset where the dataset contains three columns 'title', 'text' and 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mumbai man prays for single buttock in next bi...</td>\n",
       "      <td>while most people pray for wealth health job a...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just trying to fit into delhi culture by brand...</td>\n",
       "      <td>putting across his side of the story ex bsp mp...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in a bid to solicit support from govt lawyers ...</td>\n",
       "      <td>with fresh allegations against mj akbar croppi...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shashi tharoors new book comes with a pocket d...</td>\n",
       "      <td>shashi tharoor is ready with his new book the ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fir against delhi man for threatening couple b...</td>\n",
       "      <td>a case has been registered against former bsp ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  mumbai man prays for single buttock in next bi...   \n",
       "1  just trying to fit into delhi culture by brand...   \n",
       "2  in a bid to solicit support from govt lawyers ...   \n",
       "3  shashi tharoors new book comes with a pocket d...   \n",
       "4  fir against delhi man for threatening couple b...   \n",
       "\n",
       "                                                text label  \n",
       "0  while most people pray for wealth health job a...  Fake  \n",
       "1  putting across his side of the story ex bsp mp...  Fake  \n",
       "2  with fresh allegations against mj akbar croppi...  Fake  \n",
       "3  shashi tharoor is ready with his new book the ...  Fake  \n",
       "4  a case has been registered against former bsp ...  Fake  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./Preprocessing/news_data_final3.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GloVe Embedding\n",
    "We are using [GloVe Word Embedding](https://nlp.stanford.edu/projects/glove/) to initialize our word embedding. \n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. We have used 50 dimensional word embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/scratch/nitin/glove.6B.50d.txt','rb') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "glove_weights = np.zeros((len(lines), 50))\n",
    "words = []\n",
    "for i, line in enumerate(lines):\n",
    "    word_weights = line.split()\n",
    "    words.append(word_weights[0])\n",
    "    weight = word_weights[1:]\n",
    "    glove_weights[i] = np.array([float(w) for w in weight])\n",
    "word_vocab = [w.decode(\"utf-8\") for w in words]\n",
    "\n",
    "word2glove = dict(zip(word_vocab, glove_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of unique words in glove vectors:  0.9221395932264134\n",
      "The number of unique words are:  22322\n",
      "The first review looks like this: \n",
      "[84, 115, 56, 2450, 8, 2051, 959, 434, 3, 712, 1419, 2793, 38, 326, 37, 96, 0, 4761, 64, 21]\n",
      "And once this is converted back to words, it looks like: \n",
      "while most people pray for wealth health job and relationship hiren desai had something which even the gods would be\n"
     ]
    }
   ],
   "source": [
    "all_text = ' '.join(df.text.values)\n",
    "words = all_text.split()\n",
    "u_words = Counter(words).most_common()\n",
    "u_words_counter = u_words\n",
    "u_words_frequent = [word[0] for word in u_words if word[1]>5] # we will only consider words that have been used more than 5 times\n",
    "\n",
    "u_words_total = [k for k,v in u_words_counter]\n",
    "word_vocab = dict(zip(word_vocab, range(len(word_vocab))))\n",
    "word_in_glove = np.array([w in word_vocab for w in u_words_total])\n",
    "\n",
    "words_in_glove = [w for w,is_true in zip(u_words_total,word_in_glove) if is_true]\n",
    "words_not_in_glove = [w for w,is_true in zip(u_words_total,word_in_glove) if not is_true]\n",
    "\n",
    "print('Fraction of unique words in glove vectors: ', sum(word_in_glove)/len(word_in_glove))\n",
    "\n",
    "# # create the dictionary\n",
    "word2num = dict(zip(words_in_glove,range(len(words_in_glove))))\n",
    "len_glove_words = len(word2num)\n",
    "freq_words_not_glove = [w for w in words_not_in_glove if w in u_words_frequent]\n",
    "b = dict(zip(freq_words_not_glove,range(len(word2num), len(word2num)+len(freq_words_not_glove))))\n",
    "word2num = dict(**word2num, **b)\n",
    "word2num['<Other>'] = len(word2num)\n",
    "num2word = dict(zip(word2num.values(), word2num.keys()))\n",
    "\n",
    "int_text = [[word2num[word] if word in word2num else word2num['<Other>'] \n",
    "             for word in content.split()] for content in df.text.values]\n",
    "\n",
    "print('The number of unique words are: ', len(u_words))\n",
    "print('The first review looks like this: ')\n",
    "print(int_text[0][:20])\n",
    "print('And once this is converted back to words, it looks like: ')\n",
    "print(' '.join([num2word[i] for i in int_text[0][:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num2word[len(word2num)] = '<PAD>'\n",
    "word2num['<PAD>'] = len(word2num)\n",
    "\n",
    "for i, t in enumerate(int_text):\n",
    "    if len(t)<500:\n",
    "        int_text[i] = [word2num['<PAD>']]*(500-len(t)) + t\n",
    "    elif len(t)>500:\n",
    "        int_text[i] = t[:500]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "x = np.array(int_text)\n",
    "y = (df.label.values=='REAL').astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new delhi an etihad airways flight travelling from abu dhabi to jakarta was diverted to mumbai on wednesday morning after a passenger gave birth on board ani reported the woman was taken to hospital as soon as flight ey 474 touched down at the chhatrapati shivaji international airport'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.label=='Real'].text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'while most people pray for wealth health job and relationship hiren desai had something which even the gods would be shocked to hear the 30 year old business man from mira road who is a regularly commutes by local train asked for a single buttock in his next birth in his prayers the reason for this unique request was to enable him to sit as the fourth passenger on a local train seat meant for three speaking to faking news hiren said i have been travelling by train since last ten years and all the seats are already taken when i enter the compartment leaving me with that uncomfortable fourth seat where one of my buttock in left hanging without support by the end of the day i am left with one sore butt so my only prayer to the almighty is to give me just one buttock hiren s request struck a chord with many other passengers who have been braving heavy rush in local trains for all these years a city based cosmetic surgeon even offered to surgically remove one of hiren s buttock free of charge why wait for the next birth i will help this man in this life itself said the doctor after this news went viral on social media railways minister piyush goyal assured that he d set up a special committee to study and find out a solution to this problem the committee will be headed by a retired iit professor and a report is expected to to tabled in a year tv personality ekta kapoor moved by hiren s plight will produce a tv show titled agle janam mohe ek buttock hi dijo that will revolve around local train travelers tusshar kapoor will play the protagonist in the series'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.label=='Fake'].text.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "We are using LSTM based model to classify Fake/Real News. Architecture is as follows.\n",
    "![](images/Project_Arch.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 50)          1034300   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                10624     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,044,957\n",
      "Trainable params: 1,044,957\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word2num), 50)) # , batch_size=batch_size\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 64 samples\n",
      "Epoch 1/10\n",
      "576/576 [==============================] - 4s 7ms/step - loss: 0.6692 - acc: 0.8646 - val_loss: 0.6135 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "576/576 [==============================] - 3s 5ms/step - loss: 0.5345 - acc: 1.0000 - val_loss: 0.2519 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "576/576 [==============================] - 3s 5ms/step - loss: 0.1257 - acc: 1.0000 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "576/576 [==============================] - 3s 5ms/step - loss: 0.0327 - acc: 1.0000 - val_loss: 0.0221 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "576/576 [==============================] - 3s 6ms/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "576/576 [==============================] - 3s 5ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.0121 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "576/576 [==============================] - 3s 5ms/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "576/576 [==============================] - 3s 6ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "576/576 [==============================] - 3s 6ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "576/576 [==============================] - 3s 6ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb9e857320>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00011899125092895702, 1.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
